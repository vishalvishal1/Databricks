{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7356b34b-0f23-40a2-a802-3514477ab3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ## DF CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ddcd9d-6fb1-4fa9-84b7-75865fd4261b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"Header\", True).load(\"/Volumes/workspace/default/join/employees.csv\")\n",
    "\n",
    "df_department = spark.read.format(\"csv\").option(\"inferschema\", True).option(\"header\", True).load(\"/Volumes/workspace/default/join/departments.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#df_parquet = spark.read.format(\"parquet\").load(\"/mnt/data/sample.parquet\")\n",
    "\n",
    "#df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(\"/mnt/data/sample.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a48fa8-07fe-4d84-bf76-0e02ce44426c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755505461912}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_department)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c1eddc6-770c-4c3f-8783-805af21da16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **JOINS/ remove duplicate column from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb0d7a3-cfff-4a4f-989d-8944eab0591a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755593370976}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_leftjoin = df_employees.join(df_department,df_employees.dept_id == df_department.dept_id, \"left\").drop(df_department.dept_id)\n",
    "display(df_leftjoin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2297204a-9c6a-46a6-b318-58faae40345a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rightjoin = df_employees.join(df_department, df_employees.dept_id == df_department.dept_id, \"right\")\n",
    "\n",
    "display(df_rightjoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177bc560-b37e-4480-b389-22e4139edad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**### Broadcast join for data skewing (performance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87826df6-d1f6-45d3-ab31-982e1f342b6a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755594682962}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_broadcast = df_employees.join(broadcast(df_department), df_employees.dept_id == df_department.dept_id, \"left\").drop(df_employees.dept_id)\n",
    "\n",
    "df_broadcast.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.test\")\n",
    "\n",
    "\n",
    "df_employees.write.format(\"csv\").mode(\"overwrite\").save(\"/Volumes/workspace/default/join/transform\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939ecc8b-f1c1-4aba-9a82-7fe8cc7bbcaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write/ Partition"
    }
   },
   "outputs": [],
   "source": [
    "df_broadcast.write.format(\"csv\").mode(\"overwrite\").partitionBy(\"dept_name\").save(\"/Volumes/workspace/default/join/test\")\n",
    "\n",
    "#display(df_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed6a5ee-7608-4816-9b10-cec33fdad1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uber = spark.read.format(\"csv\").option(\"header\", True).option(\"inferschema\", True).load(\"/Volumes/workspace/default/uber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98e2a01-c4ee-43a0-af69-3fbcde11a1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ** REGEXP_REPLACE VALUE REPLACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05531bd0-91b9-4e39-8b83-4b69f9f75d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "dfclean_uber = df_uber.withColumn(\"Booking ID\", regexp_replace(col(\"Booking ID\"), '\"', '')) ## remove \"\"\"\"\"\" from  \"booking ID\" column\n",
    "\n",
    "df_clean = dfclean_uber.withColumn(\"Customer Rating\", regexp_replace(col(\"Customer Rating\"), \"null\", \"0.0\")) \n",
    "## change \"customer Rating\" \"String\" null to 0.0 to convert the string datatype to double\n",
    "\n",
    "df_cast = df_clean.withColumn(\"Customer Rating\",(col(\"Customer Rating\").cast(\"double\")))\n",
    " ## changed \"customer Rating\" column datatype from string  to double\n",
    "\n",
    "df_ubercolumn = df_cast.withColumn(\"Bookingstar\", when(col(\"Customer Rating\") >= 4.5, \"Good\").otherwise(\"bad\")) \n",
    "## creating new column \"Booking Star\" with data Good or bad  based on \"Customer Rating\" column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e28611d-acdb-4dc1-bf5b-9669a2c11361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df_uber) #before regexp_replace\n",
    "#display(dfclean_uber) #after regexp_replace\n",
    "dfclean_uber.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19435278-ca8c-431f-86be-8ba1d9ede699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **LOWERCASE DATA/ UPPERCASE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9c056c-0141-4635-a02d-d96244335c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_lower = df_uber.withColumn(\"Vehicle Type\", lower(col(\"Vehicle Type\")))\n",
    "display(df_lower)\n",
    "\n",
    "df_upper = df_lower.withColumn(\"Vehicle Type\", upper(col(\"Vehicle Type\")))\n",
    "display(df_upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf2329b-be5f-4f50-a013-408b21628a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**### DROP NULL BASED ON COLUMN / select only notnull values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea1d03b-8132-415c-9f11-f2c613f89b59",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755854385800}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_null = df_uber.filter (col(\"Booking ID\").isNotNull())\n",
    "dfclean_uber.filter(col(\"Booking ID\") == \"CNR9742182\").display()\n",
    "df_null.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2639f86a-ddf9-4b76-8c23-7057c3e9ff63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**GroupBy/Count/OrderBy** **### Find total number of rides baes on Booking Status and Vehicle Type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113139df-3e23-48c2-b168-a86d0936bf1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uber.groupby(\"Booking Status\",\"Vehicle Type\").count().orderBy(\"Booking Status\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbf7d203-2f2b-463d-8b91-0e68cbf65282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Count unique customers?\n",
    "-  select/distinct on column\n",
    "- dropDuplicate on column to get unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aee26373-d62e-4834-8c87-2ab3d365b4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96951a4f-5805-4b01-a984-b4b16776ca3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uber.select(\"Customer ID\").distinct().count()\n",
    "\n",
    "df_uber.dropDuplicates(subset=[\"Customer ID\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b24eb33-7d99-45fa-9a92-b0b5e8736c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get top 5 pickup locations with maximum completed rides.\n",
    "GroupBY/ Count/ orderBy / Filter/ show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43640212-e9ba-448b-9112-d8e2c05f8185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uber.groupBy(\"Booking Status\", \"Pickup Location\").count().orderBy(desc(\"count\")).filter(col(\"Booking Status\") == \"Completed\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94dc1df5-bfcd-4fa9-b75b-5f51e3680301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Change Datatype of \"Booking Value\",\"Ride Distance\" from string to double\n",
    "-  Calculate average ride distance & booking value per Vehicle Type\n",
    "-  group by/ avg / `display`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e48a69c-70bc-4a4d-89ca-dfab7df66ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "df_master = (\n",
    "  df_uber\n",
    "  .withColumn(\n",
    "    \"Booking Value\",\n",
    "    regexp_replace(col(\"Booking Value\"), \"null\", \"0\").cast(\"double\")\n",
    "  )\n",
    "  .withColumn(\n",
    "    \"Ride Distance\",\n",
    "    regexp_replace(col(\"Ride Distance\"), \"null\", \"0\").cast(\"double\")\n",
    "  )\n",
    ")\n",
    "\n",
    "df_master.groupBy(\"Vehicle Type\").avg(\"Booking Value\",\"Ride Distance\" ).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "547e8247-b8e9-4459-bdbd-cb1fecd2b079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repartition/ Colasce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b42a37-ead6-47c9-b64f-4bdde701480c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "dfus = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"inferschema\", \"True\").load(\"/Volumes/workspace/default/ecom/US_Accidents_March23.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703f2d74-2b85-42c0-860e-37f8020bae58",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756205779842}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfus.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a880efb9-8a2b-4c25-b8ae-6de5af276e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_withre = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"inferschema\", \"True\").load(\"/Volumes/workspace/default/ecom/US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa810e1e-f934-46ae-82b4-ee99dd97aeb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "without repartition"
    }
   },
   "outputs": [],
   "source": [
    "df_withpartition = dfus.repartition(\"State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708b5579-25b9-4257-8516-3fa9140b592c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfus.groupBy(\"State\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e24c9a-9650-4f1f-b2e4-747c20c96f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_withpartition.groupBy(\"State\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769ef771-65bc-494c-acf8-3334765923be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# 1. Aggregation by state and severity\n",
    "agg1 = dfus.groupBy(\"State\", \"Severity\") \\\n",
    "         .agg(F.count(\"*\").alias(\"incident_count\"),\n",
    "              F.avg(\"Distance(mi)\").alias(\"avg_distance\"))\n",
    "\n",
    "# 2. Aggregation by state only\n",
    "agg2 = dfus.groupBy(\"State\") \\\n",
    "         .agg(F.avg(\"Temperature(F)\").alias(\"avg_temp\"))\n",
    "\n",
    "# 3. Join both (shuffle happens here)\n",
    "joined = agg1.join(agg2, on=\"State\", how=\"inner\")\n",
    "\n",
    "# 4. Apply window function\n",
    "windowSpec = Window.partitionBy(\"State\").orderBy(F.desc(\"incident_count\"))\n",
    "final_df = joined.withColumn(\"rank_in_state\", F.rank().over(windowSpec))\n",
    "\n",
    "# Measure execution time\n",
    "t1 = time.time()\n",
    "final_df.show(10)\n",
    "t2 = time.time()\n",
    "print(\"Without repartition:\", t2 - t1, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b840591-bef0-4c4f-8109-c8fafcd52417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition data on State before heavy aggregations\n",
    "df_repart = dfus.repartition(\"State\")\n",
    "\n",
    "agg1 = df_repart.groupBy(\"State\", \"Severity\") \\\n",
    "         .agg(F.count(\"*\").alias(\"incident_count\"),\n",
    "              F.avg(\"Distance(mi)\").alias(\"avg_distance\"))\n",
    "\n",
    "agg2 = df_repart.groupBy(\"State\") \\\n",
    "         .agg(F.avg(\"Temperature(F)\").alias(\"avg_temp\"))\n",
    "\n",
    "joined = agg1.join(agg2, on=\"State\", how=\"inner\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"State\").orderBy(F.desc(\"incident_count\"))\n",
    "final_df = joined.withColumn(\"rank_in_state\", F.rank().over(windowSpec))\n",
    "\n",
    "# Measure execution time\n",
    "t1 = time.time()\n",
    "final_df.show(10)\n",
    "t2 = time.time()\n",
    "print(\"With repartition:\", t2 - t1, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a9ba74d-364d-4868-865e-3dda042c3ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"column_name\"].isNull()).show()\n",
    "\n",
    "df.filter(df[\"column_name\"].isNotNull()).show()\n",
    "\n",
    "#drop.null\n",
    "\n",
    "df.na.drop()\n",
    "df.na.drop(subset=[\"col1\", \"col2\"])\n",
    "\n",
    "\n",
    "df.na.fill(\"unknown\")  # if columns are strings\n",
    "df.na.fill(0)          # if columns are numeric\n",
    "\n",
    "df.na.fill({\"age\": 0, \"name\": \"N/A\"})\n",
    "\n",
    "#replace with condition\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_na = df.withColumn(\"age_cleaned\", when(col(\"age\").isNull(), 0).otherwise(col(\"age\")))\n",
    "\n",
    "\n",
    "col1    col2      col3\n",
    "car   electric   sedan\n",
    "car    electric   null\n",
    "car    petrol     suv\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Assuming your DataFrame is called df\n",
    "df_updated = df.withColumn(\n",
    "    \"col3\",\n",
    "    when(\n",
    "        (col(\"col1\") == \"car\") & (col(\"col2\") == \"electric\") & col(\"col3\").isNull(),\"sedan\").otherwise(col(\"col3\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a30a894-11a7-4659-9900-6552cf08d914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#INCREMENTAL LOGIC\n",
    "#If Using Delta Lake + MERGE (for UPSERT)\n",
    "#To handle both new and updated rows, use MERGE INTO (available in Delta Lake):\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_target = DeltaTable.forName(spark, \"target_table\")\n",
    "\n",
    "delta_target.alias(\"target\").merge(\n",
    "    source_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"  # join condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5134886673624315,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks worknote",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
